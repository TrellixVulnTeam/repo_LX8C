{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Part C: Trying alternative classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a skeleton for trying alternative classifiers on the basketball dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier, BaggingClassifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Method | Accuracy | Error rate |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| AdaBoostClassifier | 0.5962765527824893 | 0.4037234472175107 |\n",
    "| Nearest Neighbors | 0.5440998303852678 | 0.45590016961473223 |\n",
    "| Naive Bayes (Gaussian) | 0.5679266618205314 | 0.4320733381794685 |\n",
    "| Random Forests(n_estimators=1000, max_features=5)| 0.5869073580486229 | 0.4130926419513771 |     \n",
    "|GradientBoosting| 0.6015669170503191 | 0.39843308294968094 \n",
    "|GradientBoosting(learning_rate=0.001,n_estimators=10000)| 0.6024553751716339 | 0.39754462482836606 |\n",
    "|VotingClassifier(estimators=[('AdaBoost', AdaBoostClassifier(n_estimators=100)), ('RandomForest', RandomForestClassifier(n_estimators=1000, max_features=5)), ('GradientBoosting', GradientBoostingClassifier(learning_rate=0.001,n_estimators=10000))], voting='hard')| 0.6016880704304983 | 0.3983119295695017 |\n",
    "|GradientBoostingClassifier(max_depth=4)| 0.6005573055488248 | 0.3994426944511752 | Current best |\n",
    "|GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500, min_samples_leaf=50, max_depth=4, max_features='sqrt', subsample=0.8, random_state=8)| 0.60314191099265 | 0.39685808900735 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define, as done in Practicum 6, a data loading in a way to obtain the attributes set and class labels for each the training and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ATTRS = [\"LOCATION\", \"W\", \"FINAL_MARGIN\", \"SHOT_NUMBER\", \"PERIOD\", \"GAME_CLOCK\", \"SHOT_CLOCK\", \"DRIBBLES\", \"TOUCH_TIME\",\n",
    "         \"SHOT_DIST\", \"PTS_TYPE\", \"CLOSE_DEF_DIST\", \"SHOT_RESULT\"]\n",
    "ATTRS_WO_CLASS = 12\n",
    "\n",
    "def load_data(filename):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    with open(filename, 'rt') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        i = 0\n",
    "        for row in csvreader:\n",
    "            if len(row) == ATTRS_WO_CLASS + 1:\n",
    "                i += 1\n",
    "                instance = [row[i] for i in range(ATTRS_WO_CLASS)]  # first ATTRS_WO_CLASS values are attributes\n",
    "                label = row[ATTRS_WO_CLASS]  # (ATTRS_WO_CLASS + 1)th value is the class label\n",
    "                if i % 5 == 0:  # test instance\n",
    "                    test_x.append(instance)\n",
    "                    test_y.append(label)\n",
    "                else:  # train instance\n",
    "                    train_x.append(instance)\n",
    "                    train_y.append(label)\n",
    "            \n",
    "                \n",
    "                    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test(filename):\n",
    "    test_z = []\n",
    "    with open(filename, 'rt') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        i = 0\n",
    "        for row in csvreader:\n",
    "            test_z.append(row)\n",
    "    return test_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can use it to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = load_data(\"data/basketball.train.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_z = load_test(\"data/basketball.test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn needs that all the attribute values to be numeric. This is, we need to binarize all the non-numeric attribute values, to obtain vectors: records having only numbers. The `DictVectorizer` class provided by scikit-learn allows to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind that each `train_x` and `test_x` are a list of lists.\n",
    "\n",
    "We just need to obtain from each a list of dictionaries (as done in previous practica where each record was a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dicts_train_x = []\n",
    "for x in train_x:\n",
    "    d = {}\n",
    "    for i, attr in enumerate(ATTRS):\n",
    "        if i < len(ATTRS) - 1: # we removed class from train_x elems\n",
    "            val = x[i]\n",
    "            # TODO: save as floats the values for the already-numeric attributes from dataset, keep the rest as the strings they are\n",
    "            if i not in [0, 1, 4, 10]:\n",
    "                val = float(val)\n",
    "            d[attr]=val\n",
    "    dicts_train_x.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `fit_transform` method of the vectorizer binarizes the non-numeric attributes in the list of dictionaries, and returns the vector we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_train = DictVectorizer()\n",
    "vec_train_x = vectorizer_train.fit_transform(dicts_train_x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do similarly for vectorizing `test_x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dicts_test_x = []\n",
    "# TODO\n",
    "for x in test_x:\n",
    "    d = {}\n",
    "    for i, attr in enumerate(ATTRS):\n",
    "        if i < len(ATTRS) - 1: # we removed class from train_x elems\n",
    "            val = x[i]\n",
    "            # TODO: save as floats the values for the already-numeric attributes from dataset, keep the rest as the strings they are\n",
    "            if i not in [0, 1, 4, 10]:\n",
    "                val = float(val)\n",
    "            d[attr]=val\n",
    "    dicts_test_x.append(d)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_test = DictVectorizer()\n",
    "vec_test_x = vectorizer_test.fit_transform(dicts_test_x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dicts_test_z = []\n",
    "for x in test_z:\n",
    "    d = {}\n",
    "    for i, attr in enumerate(ATTRS):\n",
    "        if i < len(ATTRS)-1:\n",
    "            val = x[i]\n",
    "            # TODO: save as floats the values for the already-numeric attributes from dataset, keep the rest as the strings they are\n",
    "            if i not in [0, 1, 4, 10]:\n",
    "                val = float(val)\n",
    "            d[attr]=val\n",
    "    dicts_test_z.append(d)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_test = DictVectorizer()\n",
    "vec_test_z = vectorizer_test.fit_transform(dicts_test_z).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having `evaluate` defined somewhere, we are ready to learn and apply the model, similarly to Task 3 of Practicum 6. But here, we use the vectors recently obtained for the input sets. E.g., for Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(predictions, true_labels):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == true_labels[i]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "\n",
    "    print(\"\\tAccuracy:   \", correct / len(predictions))\n",
    "    print(\"\\tError rate: \", incorrect / len(predictions))\n",
    "    print(\"|\",correct / len(predictions),\"|\",incorrect / len(predictions), \"|\")\n",
    "    return correct\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "**learning in progress**\n",
      "Done\n",
      "Nearest Neighbors\n",
      "**learning in progress**\n",
      "Done\n",
      "Naive Bayes (Gaussian)\n",
      "**learning in progress**\n",
      "Done\n",
      "Random Forests\n",
      "**learning in progress**\n",
      "Done\n",
      "GradientBoosting\n",
      "**learning in progress**\n",
      "Done\n",
      "ExtraTrees\n",
      "**learning in progress**\n",
      "Done\n",
      "Bagging\n",
      "**learning in progress**\n",
      "Done\n",
      "Voting\n",
      "**learning in progress**\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifiers = {\"AdaBoostClassifier\": AdaBoostClassifier(n_estimators=100),\n",
    "               \"Nearest Neighbors\": KNeighborsClassifier(n_neighbors=200),\n",
    "               \"Naive Bayes (Gaussian)\": GaussianNB(),\n",
    "               \"Random Forests\": RandomForestClassifier(),  # number of trees in the forest, and maximum number of features in each tree\n",
    "               \"GradientBoosting\": GradientBoostingClassifier(learning_rate=0.1, min_samples_split=600, min_samples_leaf=55, max_depth=4, max_features='sqrt', subsample=0.8, random_state=8),\n",
    "               \"ExtraTrees\":ExtraTreesClassifier(), \n",
    "               \"Bagging\":BaggingClassifier(),\n",
    "               \"Voting\":VotingClassifier(estimators=[(\"Nearest Neighbors\", KNeighborsClassifier(n_neighbors=200)),(\"Naive Bayes (Gaussian)\", GaussianNB()),('AdaBoost', AdaBoostClassifier()), ('RandomForest', RandomForestClassifier()), ('GradientBoosting', GradientBoostingClassifier())], voting='hard', weights=[4,5,2,5,2]), \n",
    "              }\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(name)\n",
    "    print(\"**learning in progress**\")\n",
    "    clf.fit(vec_train_x, train_y)\n",
    "    predictions = clf.predict(vec_test_z)    #Evaluate with test_x, build model with test_z\n",
    "#     evaluate(predictions,test_y)     #uncomment when testing classifiers with vec_test_x in predicitions\n",
    "    OUTPUT_FILE = \"data/prediction{}.csv\".format(name)\n",
    "    with open(OUTPUT_FILE, \"w\") as fout:\n",
    "        fout.write(\"Id,Target\\n\")\n",
    "        for i, record in enumerate(predictions):           \n",
    "            fout.write(str(i+1) + \",\" + record + \"\\n\")\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
